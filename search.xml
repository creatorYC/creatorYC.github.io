<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Logistic 回归与梯度上升算法]]></title>
    <url>%2F2018%2F04%2F23%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[引言最近在看《机器学习实战》，这本书将所有的算法都用 Python 代码实现了，非常具有实战性，但对于算法原理的讲解则比较粗略，也没有详细的数学推导过程，所以很多哪怕几行简单的代码，也得找资料理解好久。 Logistic 回归与梯度上升算法Logistic 回归 Logistic 回归用于二分类问题，根据现有数据对分类边界线建立回归公式，并以此进行分类。Logistic 回归本质上是一个基于条件概率的判别模型，利用了 Sigmoid 函数值域在(0, 1)之间的特性。Sigmoid 函数的计算公式为： g(z)=\frac{1}{1+e^{-z}}通过 Sigmid 函数计算出结果，若结果大于0.5，则为类别1，反之属于类别0。由于数据有多个特征，因此函数的输入用向量表示如下： \theta_0+\theta_1x_1+\cdots+\theta_nx_n=\sum_{i=0}^{n} {\theta_ix_i}=\theta^Tx则函数计算公式变为： h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}如果我们求出了合适的参数向量 $\theta$，以及样本x，那么就可以根据上述公式，计算出概率值，从而判断x所属的分类。 那么如何求出合适的参数向量$\theta$呢？ 上述 $h_\theta(x)$ 函数具有特殊含义，它表示结果取1的概率，因此对于输入的x分类结果为类别1和类别0的概率为： P(y=1|x;\theta)=h_\theta(x)P(y=0|x;\theta)=1-h_\theta(x)将两个公式合并为一个即为： P(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}取最大似然函数为： L(\theta)=\prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}其中，m为样本总数，$x^{(i)}$为第i个样本,$y^{(i)}$为第i个样本的类别。对最大似然函数取对数为： l(\theta)=log L(\theta)=\sum_{i=1}^m y^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))其中，满足使$l(\theta)$值最大的$\theta$值，即为所求的参数值。 梯度上升算法 现在如何求满足使$l(\theta)$值最大的$\theta$值呢？这就需要用到梯度上升算法。 根据梯度上升算法可得$\theta$的更新过程： \theta_j=\theta_j+\alpha \frac{\partial{l(\theta)}}{\partial{\theta_j}}其中， $\alpha$为步长。 \begin{align*} \frac{\partial{l(\theta)}}{\partial{\theta_j}}&=\sum_{i=1}^m \left(y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) \frac{\partial{g(\theta^T x^{(i)})}}{\partial{\theta_j}} \\ &=\sum_{i=1}^m \left( y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) g(\theta^T x^{(i)})\left(1-g(\theta^T x^{(i)}) \right) \frac{\partial{\theta^T x^{(i)}}}{\partial{\theta_j}} \\ &= \sum_{i=1}^m \left( y^{(i)}(1-g(\theta^T x^{(i)}))-(1-y^{(i)})g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-h_\theta (x^{(i)}) \right)x^{(i)}_j \end{align*}上述公式求解过程中用到如下换算公式： f(x)=\frac{1}{1+e^{g(x)}}\begin{align*} \frac{\partial{f(x)}}{x}&=\frac{1}{\left( 1+e^{g(x)} \right)^2}e^{g(x)}\frac{\partial{g(x)}}{\partial{x}} \\ &= \frac{1}{1+e^{g(x)}} \frac{e^{g(x)}}{1+e^{g(x)}} \frac{\partial{g(x)}}{\partial{x}} \\ &= f(x)\left(1-f(x)\right) \frac{\partial{g(x)}}{\partial{x}} \end{align*}\frac{\partial{\theta^T x}}{\partial{\theta_j}}=\frac{\partial{(\theta_1 x_1+\cdots+\theta_n x_n)}}{\partial{\theta_j}}=x_j因此，$\theta$的更新过程为： \theta_j=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right) x^{(i)}_j梯度上升过程的向量化 训练数据的矩阵形式如下，x的每一行为一个训练样本，每一列为不同的特征值。 x=\left[ \begin{matrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \end{matrix} \right]= \left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right], y=\left[ \begin{matrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{matrix} \right]参数向量$\theta$的矩阵形式为： \theta=\left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]先求 $x\cdot\theta$并记结果为A: A=x\cdot\theta=\left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right] \cdot \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]= \left[ \begin{matrix} \theta_0 x_0^{(1)}+\theta_1 x_1^{(1)}+\cdots+\theta_n x_n^{(1)} \\ \theta_0 x_0^{(2)}+\theta_1 x_1^{(2)}+\cdots+\theta_n x_n^{(2)} \\ \vdots \\ \theta_0 x_0^{(m)}+\theta_1 x_1^{(m)}+\cdots+\theta_n x_n^{(m)} \\ \end{matrix} \right]再求 $y-h_\theta(x)$ 并记结果为E: E=y-h_\theta(x)= \left[ \begin{matrix} g\left( A^{(1)} \right)-y^{(1)} \\ g\left( A^{(2)} \right)-y^{(2)} \\ \vdots \\ g\left( A^{(m)} \right)-y^{(m)} \end{matrix} \right]= \left[ \begin{matrix} e^{(1)} \\ e^{(2)} \\ \vdots \\ e^{(m)} \end{matrix} \right]= g(A)-y由以上式子可知，$y-h_\theta(x)$ 可由 $g(A)-y$ 一次求得，带入 $\theta$ 的更新过程可得： \begin{align*} \theta_j&=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right)x_j^{(i)} \\ &= \theta_j+\alpha \sum_{i=1}^m e^{(i)} x^{(i)} \\ &= \theta_j+\alpha \left( x_j^{(1)},x_j^{(2)}, \cdots,x_j^{(m)} \right) \cdot E \end{align*}综合起来就是： \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]= \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]+\alpha \cdot \left[ \begin{matrix} x_0^{(1)}&,x_0^{(2)}&,\cdots&,x_0^{(m)} \\ x_1^{(1)}&,x_1^{(2)}&,\cdots&,x_1^{(m)} \\ \vdots&\vdots&\vdots&\vdots \\ x_n^{(1)}&,x_n^{(2)}&,\cdots&,x_n^{(m)} \end{matrix} \right] \cdot E = \theta+\alpha \cdot x^T \cdot EPython代码如下:1234567891011121314151617181920# 阶跃函数def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 使用梯度上升算法计算最佳回归系数def gradientAscent(dataMatIn, classLabels): # 转换为Numpy矩阵类型 dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).T # 矩阵转置 m, n = shape(dataMatrix) alpha = 0.001 # 向目标移动的步长 numCycles = 500 # 迭代次数 weights = ones((n, 1)) # 计算真实类别与预测类别的差值, 按照该差值的方向调整回归系数 for k in range(numCycles): h = sigmoid(dataMatrix * weights) # 矩阵相乘 error = (labelMat - h) # 向量相减 weights = weights + alpha * dataMatrix.T * error return weights 总结《机器学习实战》中短短数十行代码中隐藏了太多的细节，如果不是查阅很多资料，对于初学者来说真的很难弄懂，以上过程还只是最简单的Logistic回归，书上还提供了随机Logistic回归和优化后的随机Logistic回归，看得也是云里雾里 -_-!]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>logistic回归</tag>
        <tag>梯度上升算法</tag>
      </tags>
  </entry>
</search>
