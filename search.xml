<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Chrome 扩展 -- hanpin]]></title>
    <url>%2F2018%2F06%2F13%2Fhanpin%2F</url>
    <content type="text"><![CDATA[引言最近在学习 JavaScript，想着做点东西练手，之前一直对开发 Chrome 扩展很好奇，正好趁着这个机会想做点东西出来，因此开发了 hanpin。 Chrome 扩展使用 JavaScript 语言开发，配合使用 HTML 和 CSS，只要遵循 Chrome 扩展开发要求，其他就跟编写普通的网页差不多。再辅以 Chrome 扩展专用的一些 API，你就能做出你能想到的一切效果，还是挺有意思的。 扩展介绍hanpin 是一个划词注音的扩展，在页面上选中一些中文，即可展示出对应的拼音，用户还可以在选项页面进行设置是否开启多音字功能和是否开启分词功能（分词功能会降低一些效率），类似于现在很流行的划词翻译的功能。略有遗憾的是选用的拼音库对生僻字的支持不太理想。 Chrome 扩展开发简介manifest.json扩展被安装后，Chrome 浏览器就会读取扩展中的 manifest.json 文件。这个文件的文件名固定为 manifest.json，内容是按照一定格式描述的扩展相关信息，如扩展名称、版本、更新地址、请求的权限、扩展的UI界面入口等等，这样 Chrome 就可以知道在浏览器中如何呈现这个扩展，以及这个扩展如何同用户进行交互。下面列出主要的框架12345678910111213141516171819202122232425262728293031323334&#123; "manifest_version": 2, "name": "拼音", "version": "1.0", "description": "汉字拼音", "icons": &#123;&#125;, "browser_action": &#123; "default_icon": &#123; "19": "images/icon19.png", "38": "images/icon38.png" &#125;, "default_title": "汉字拼音" "default_popup": "pages/popup.html" &#125;, "background": &#123; "scripts": ["scripts/back_bundle.js"], "persistent": true &#125;, "content_scripts": [ &#123; "matches": ["http://*/*", "https://*/*"], "js": ["scripts/selection.js"], "css": ["styles/style.css"] &#125; ], "options_page": "pages/options.html", "permissions": [ "contextMenus", "storage" ], "web_accessible_resources": [ "images/icon.png" ]&#125; 上述文件中，name 定义了扩展的名称，version 定义了扩展的版本，description 定义了扩展的描述，icons 定义了扩展相关图标文件的位置，browser_action 指定扩展的图标放在 Chrome 的工具栏中，browser_action 中的 default_icon 属性定义了相应图标文件的位置，default_title 定义了当用户鼠标悬停于扩展图标上所显示的文字，default_popup 则定义了当用户单击扩展图标时所显示页面的文件位置，background 可以使扩展常驻后台，其中的scripts 属性会在扩展启动时自动创建一个包含所有指定脚本的页面，persistent 属性定义了常驻后台的方式——当其值为 true 时，表示扩展将一直在后台运行，无论其是否正在工作；当其值为 false 时，表示扩展在后台按需运行，如非必要，请将 persistent 设置为 false。persistent 的默认值为 true。content_scripts 属性值为数组类型，数组的每个元素可以包含 matches、exclude_matches、css、js、run_at 等属性，其中 matches 属性定义了哪些页面会被注入脚本，exclude_matches 则定义了哪些页面不会被注入脚本，css 和 js 对应要注入的样式表和 JavaScript，run_at 定义了何时进行注入，options_page 允许用户对扩展程序进行一些个性化的设置，因此需要提供一个选项页面。permissions 是程序需要申请的权限，比如需要浏览器存储数据、操作当前页面等等。web_accessible_resources 是扩展程序中需要使用的资源文件，比如图片等内容。 popuppopup 是用户点击浏览器上扩展的图标时的弹框，可以在里面介绍这个扩展的功能或者展示一些其他的数据（这可是给自己打广告的好地方啊）。 content_scripts通过 Chrome 扩展我们可以对用户当前浏览的页面进行操作，实际上就是对用户当前浏览页面的 DOM 进行操作。通过manifest.json 中的 content_scripts 属性可以指定将哪些脚本何时注入到哪些页面中，当用户访问这些页面后，相应脚本即可自动运行，从而对页面 DOM 进行操作。在我的程序中，需要在 content_scripts 中完成的功能是选中文字、弹出注音图标、点击图标出现对应的拼音。在相应的 JavaScript 中需要编写的代码主要有选中文字、发送消息到 background、加载图标以及监听各种鼠标事件。其中页面间的通信需要用到 Chrome 提供的 chrome.runtime.sendMessage API，Chrome 提供了4个有关扩展页面间相互通信的接口，分别是 runtime.sendMessage、runtime.onMessage、runtime.connect 和 runtime.onConnect。 backgroundChrome 允许扩展应用在后台常驻一个页面以实现这样的功能，类似于服务器端程序，其他页面程序可以通过 Chrome 的通信机制跟 background 之间通信。我的程序中，background 主要负责接收 content_scripts 中发过来的需要注音的文字，然后将对应的拼音作为消息回送给 content_scripts，并在这个过程中从 localStorage 中取出用户自定义的设置，以决定是否开启多音字或者分词功能。 options_page在选项页面，可以为用户开发一下自定义的功能，用户自定义的数据可以保存在 localStorage 中（类似于网站开发中的 Cookies），options_page 需要提供一个 HTML 页面以及对应的 JavaScript。我的程序中，options_page 主要提供了一个页面，上面有一些复选框，用于用户决定是否开启多音字功能和分词的功能。 程序运行截图 运行界面 设置界面 补充说明我在程序中使用了 ES6 标准支持的 require、import 等关键字，由于初学 JavaScript，调试的时候才发现浏览器里识别不了这些关键字，折腾了好久才了解了 browserify 这个工具，将这些不认识的关键字“翻译”为浏览器认识的标记，才使得扩展程序成功的运行起来。 推荐的参考资料除了 Chrome 提供的官方参考文档以外，还想推荐一个中文的教程，可以快速的让初学者从整体上掌握开发 Chrome 扩展的一般流程以及对扩展开发有个宏观的理解。资料地址为 Chrome扩展及应用开发（首发版） 欢迎 star]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Chrome-Extension</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F05%2F17%2FadaBoost%2F</url>
    <content type="text"><![CDATA[引言AdaBoost, 是“Adaptive Boosting(自适应增强)”的缩写，它是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。 它的自适应性在于：前一个基本分类器分错的样本的权值在下一轮迭代时会上升，分对的样本的权值在下一轮迭代时会降低，加权后的全体样本再次被用来训练下一个基本分类器。每一轮迭代都会产生一个新的基本分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 AdaBoost 算法原理AdaBoost 算法分为三步： 初始化训练数据的权值分布。如果有 N 个样本，则最开始时每个样本的权值都相同，均为： $\frac{1}{N}$。 训练弱分类器。训练过程中，如果某个样本点已经被正确的分类，那么在构造下一个训练集时它的权值就会降低，相反，如果某个样本点分类错误，则它的权值将会升高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代下去。 将每次迭代训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权值，使其在最终的分类函数中起较大的决定作用，减小分类误差率大的弱分类器的权值，使其在最终的分类函数中起较小的决定作用，即误差率低的弱分类器在最终的分类器中占较大的权重。 AdaBoost 算法流程给定一个训练数据集 $ T={(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)} $， 其中，$x \in \chi$，实例空间 $\chi \subset R^n$，$y_i \in {-1, +1}$，算法流程如下： 首先，初始化训练数据的权值分布。每一个训练样本最开始都被赋予相等的权值：$\frac{1}{N}$D_1 = (w_{1,1}, w_{1,2}, \cdots, w_{1,i}, \cdots, w_{1,N}), w_{1,i} = \frac{1}{N}, i=1,2,\cdots,N 进行多次迭代，用 $m = 1,2, \cdots, M$ 表示迭代的轮次。 使用具有权值分布$D_m$的训练集学习，得到基本分类器(选取让误差率最低的阈值来设计基本分类器):G_m(x): x \rightarrow \{-1. +1\} 计算 $G_m(x)$ 在训练数据集上的误差率，e_m = P(G_m(x_i) \neq y_i) = \sum_{G_m(x_i) \neq y_i} w_{mi}由上述式子可知，$G_m(x)$在训练数据集上的误差率 $e_m$ 就是被$G_m(x)$误分类样本的权值之和。 计算 $G_m(x)$ 的系数，$\alpha_m$表示$G_m(x)$在最终的分类器中的重要程度，即得到基本分类器在最终分类器中的权重：\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}由上述式子可知，$e_m \leq \frac{1}{2}$ 时，$\alpha_m \geq 0$，且 $\alpha_m$ 随着 $e_m$ 的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 更新训练数据集的权值分布，得到样本新的权值分布，用于下一轮迭代：D_{m+1} = (w_{m+1,1}, \cdots, w_{m+1,i}, \cdots, w_{m+1, N}),w_{m+1,i} = \frac{w_{m,i}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), i=1,2,\cdots,N其中，$Zm$ 是规范化因子，使得 $D{m+1}$ 成为一个概率分布：Z_m = \sum_{i=1}^N w_{m,i} exp(-\alpha_m y_i G_m(x_i))这样就使得被基本分类器$G_m(x)$误分类样本的权值增大，而被正确分类样本的权值减小。 组合各个弱分类器。 f(x) = \sum_{m=1}^M \alpha_m G_m(x)从而得到最终的分类器如下： G(x) = sign(f(x)) = sign \left( \sum_{m=1}^M \alpha_m G_m(x) \right)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM 初探]]></title>
    <url>%2F2018%2F04%2F28%2FSVM%2F</url>
    <content type="text"><![CDATA[引言SVM — 支持向量机，它是一个二类分类器。在样本数据中，找出一个分割超平面(因为数据集有很多特征，所以是多维的，因此是个分割面)，分布在分割超平面两侧的数据属于不同的类别。为了找出最佳的分割超平面，需要满足一些条件，即离分割超平面距离最近的点的间隔尽可能大。而离分割超平面最近的那些点，就被称为支持向量。现在的目标就是求解这个最佳的分割超平面。 函数间隔与几何间隔分割超平面的方程可以写为：$ \omega^Tx+b=0 $，在分割面的方程确定后，$|\omega^Tx+b|$ 能相对地表示$x$距离超平面的远近，并且，可以通过判断$\omega^Tx+b$的符号与类标记$y$的符号是否一致来表示分类是否正确($y$为1对应$\omega^Tx+b&gt;0$，$y$为-1对应$\omega^Tx+b&lt;0$),所以可以用$y(\omega^Tx+b)$来表示分类的正确性以及某种意义下的距离超平面的距离。这就是所谓的“函数间隔”(这个结果可以保证间隔的值为非负)。但是这样定义的函数间隔有问题，比如说，将$b$和$\omega$成比例的扩大$k$倍，其超平面$k\omega+kb=0$跟之前的超平面是一样的，但是间隔$y(k\omega+kb)$却较之前扩大了$k$倍。因此需要对法向量$\omega$规范化，从而定义“几何间隔”： \gamma = \frac{ y(\omega^Tx+b) }{||\omega||}最大间隔分类器对于一个数据点进行分类时，离分割面的距离越大越可信。对于一个包含n个点的数据集，间隔为n个点中最小的那个，为了使得分类的可信度高，我们选择的超平面应该最大化这个间隔。因此我们的目标函数为： \max\ \gamma需要满足的限制条件为： y_i (\omega^T x_i+b) = \gamma_i \geq \gamma , i=1,\cdots,n为了方便推导，令函数间隔$ y(\omega^Tx+b) = 1$，因此，目标函数为： \max\ \frac{1}{||\omega||} , s.t. \ y_i (\omega^T x_i+b) \geq 1这个问题等价于（为了方便求解，加上了平方，还有一个系数）： \min\ \frac{1}{2} ||\omega||^2 , s.t. \ y_i (\omega^T x_i+b) \geq 1这是一个二次优化问题——目标函数是二次的，约束条件是线性的，这个优化问题可以用拉格朗日乘数法求解(大学学的数学还真的很有用，哈哈~)，因此，最终的函数形式为： L(\omega, b, \alpha) = \frac{1}{2} ||\omega||^2 - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1)然后我们令： \theta(\omega) = \max \limits_{\alpha_i \geq 0} L(\omega, b, \alpha)容易验证，当某个约束条件不满足时，例如 $ y_i(\omega^Tx_i+b) \le 1 $，那么我们显然有 $ \theta(\omega) = \infty $，而当所有约束条件都满足时，则有 $ \theta(\omega) = \frac{1}{2} ||\omega||^2 $ ，即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2} ||\omega||^2$ 实际上等价于直接最小化$\theta(\omega)$。现在，目标函数变成了： \min \limits_{\omega, b} \theta(\omega) = \min \limits_{\omega, b} \max \limits_{\alpha_i \geq 0}L(\omega, b, \alpha) = P这里用 $P$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。现在把最小和最大的位置交换一下： \max \limits_{\alpha_i \geq 0} \min \limits_{\omega, b} L(\omega, b, \alpha) = D交换以后的问题不再等价于原问题，这个新问题的最优值用 $D$来表示。并且有 $D \leq P $，(这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！) 总之，第二个问题的最优值 $D$在这里提供了一个第一个问题的最优值 $P$的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。现在来求解这个问题。先让 $L$ 最小化，直接让$L$对$\omega$和$b$求导， \frac{\partial L}{\partial \omega} = \omega - \sum_{i=1}^n \alpha_i y_i x_i = 0 \Rightarrow \omega = \sum_{i=1}^n \alpha_i y_i x_i\frac{\partial L}{\partial b} = - \sum_{i=1}^n \alpha_i y_i = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0带回 $L$ 得到： \begin{align*} L(\omega, b, \alpha) &= \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ &= \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \end{align*}此时得到关于 $\alpha$ 的优化问题： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_js.t. \alpha_i \geq 0, i=1,\cdots, n , \sum_{i=1}^n \alpha_i y_i = 0我们对一个数据点$x$进行分类的时候，实际上是通过将$x$代入到 $f(x)=\omega^Tx+b$，根据结果的正负来判断$x$所属的分类，而 $\omega = \sum_{i=1}^n \alpha_i y_i x_i$，因此： f(X) = (\sum_{i=1}^n \alpha_i y_i x_i)^T x + b = \sum_{i=1}^n \alpha_i y_i \langle x_i, x \rangle + b这里有趣之处在于，对于新点$x$的预测，只需要计算它与训练数据点的内积即可。另外，所谓的支持向量也在这里显示出来—事实上，所有的非支持向量所对应的系数$\alpha$都为0，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。为什么非支持向量对应的$\alpha$等于零呢？之前的目标函数为： \max \limits_{\alpha_i \geq 0} L(\omega, b, \alpha) = \max \limits_{\alpha_i \geq 0} \frac{1}{2} ||\omega||^2 - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1)注意到如果$x_i$是支持向量的话，上式中$(y_i(\omega^Tx_i+b)-1)$部分是等于 0 的（因为支持向量的函数间隔等于 1 ），而对于非支持向量来说，函数间隔会大于 1 ，因此$(y_i(\omega^Tx_i+b)-1)$是大于零的，而$\alpha_i$又是非负的，为了满足最大化，$\alpha_i$必须等于 0 。 Kernel前面介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。但是大部分数据集都是线性不可分的，因此，对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。前面我们得到的分类函数为： f(x) = \sum_{i=1}^n \alpha_i y_i \langle x_i, x \rangle + b现在则为映射后的空间，即： f(x) = \sum_{i=1}^n \alpha_i y_i \langle \phi(x_i), \phi(x) \rangle + b因此，建立非线性学习器分为两步： 首先使用一个非线性映射将数据变换到一个特征空间 然后在特征空间使用线性学习器分类。 因为直接将低维空间映射到高维空间的函数$\phi(x)$可能很难计算，因此如果有一种方式可以在特征空间中直接计算内积 $\langle \phi(x_i), \phi(x) \rangle$，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个非线性的学习器，这样直接计算法的方法称为核函数方法： K(x, z) = \langle \phi(x), \phi(z) \rangle核函数能简化映射空间中的内积运算。现在，分类函数为： \sum_{i=1}^n \alpha_i y_i K(x,x_i) + b其中 $\alpha$ 由如下问题计算而得： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j)s.t. \alpha_i \geq 0, i=1,\cdots, n , \sum_{i=1}^n \alpha_i y_i = 0这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算。核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。对于任意一个映射，构造出合适的核函数很困难，因此通常人们会从一些常用的核函数中选择，如： 高斯核函数：K(x1, x2) = exp \left( -\frac{||x1-x2||^2}{2\sigma^2} \right)这是一个采用 向量 作为自变量的函数，能够基于向量距离运算输出一个标量，其中，$\sigma$ 是用户定义的用于确定到达率或者函数值跌落到0的速度参数，高斯核函数将数据从其特征空间映射到无穷维空间。 Outliers如果不是因为数据本身是非线性结构的，而只是因为数据有噪音，对于这种偏离正常位置很远的数据点(outlier) ，在我们原来的 SVM 模型里，可能造成很大的影响。为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。具体来说，原来的约束条件为： y_i(\omega^T x_i+b) \geq 1, i=1,\cdots,n现在变为： y_i(\omega^T x_i+b) \geq 1-\xi_i, i=1,\cdots,n其中 $\xi_i \geq 0$ 称为松弛变量(slack variable)，对应数据点$x_i$允许偏离的函数间隔的量。当然，如果我们任由 $\xi_i$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi_i$的总和也要最小： \min \frac{1}{2} ||\omega^2|| + C\sum_{i=1}^n \xi_i其中$C$是一个参数，用于控制目标函数中两项（“寻找间隔最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中$\xi_i$是需要优化的变量，而$C$是一个事先确定好的常量。完整地写出来为： \min \frac{1}{2} ||\omega^2|| + C\sum_{i=1}^n \xi_is.t. y_i(\omega^T x_i+b) \geq 1-\xi_i, i=1,\cdots,n\xi_i \geq 0, i=1,\cdots,n用之前的方法将限制加入到目标函数中，为： L(\omega, b, \alpha) = \frac{1}{2} ||\omega||^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1+\xi_i) - \sum_{i=1}^n \gamma_i \xi_i像前面一样，先求导： \frac{\partial L}{\partial \omega}=0 \Rightarrow \omega=\sum_{i=1}^n \alpha_i y_i x_i\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0\frac{\partial L}{\partial \xi_i}=0 \Rightarrow C-\alpha_i-\gamma_i=0, i=1,\cdots,n将 $\omega$带回$L$并化简可得到与之前一样的目标函数： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangle不过，因为有 $ C-\alpha_i-\gamma_i=0 $ 和 $ \gamma_i \geq 0 $，因此有 $ C \geq \alpha_i \geq 0 $，所以最终问题写为： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangles.t. 0 \leq \alpha_i \leq C, i=1,\cdots,n\sum_{i=1}^n \alpha_i y_i = 0这与之前问题的唯一区别就是$\alpha$多了一个上限$C$。现在，一个完整的、可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机终于介绍完毕了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic 回归与梯度上升算法]]></title>
    <url>%2F2018%2F04%2F23%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[引言最近在看《机器学习实战》，这本书将所有的算法都用 Python 代码实现了，非常具有实战性，但对于算法原理的讲解则比较粗略，也没有详细的数学推导过程，所以很多哪怕几行简单的代码，也得找资料理解好久。 Logistic 回归与梯度上升算法Logistic 回归 Logistic 回归用于二分类问题，根据现有数据对分类边界线建立回归公式，并以此进行分类。Logistic 回归本质上是一个基于条件概率的判别模型，利用了 Sigmoid 函数值域在(0, 1)之间的特性。Sigmoid 函数的计算公式为： g(z)=\frac{1}{1+e^{-z}}通过 Sigmid 函数计算出结果，若结果大于0.5，则为类别1，反之属于类别0。由于数据有多个特征，因此函数的输入用向量表示如下： \theta_0+\theta_1x_1+\cdots+\theta_nx_n=\sum_{i=0}^{n} {\theta_ix_i}=\theta^Tx则函数计算公式变为： h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}如果我们求出了合适的参数向量 $\theta$，以及样本x，那么就可以根据上述公式，计算出概率值，从而判断x所属的分类。 那么如何求出合适的参数向量$\theta$呢？ 上述 $h_\theta(x)$ 函数具有特殊含义，它表示结果取1的概率，因此对于输入的x分类结果为类别1和类别0的概率为： P(y=1|x;\theta)=h_\theta(x)P(y=0|x;\theta)=1-h_\theta(x)将两个公式合并为一个即为： P(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}取最大似然函数为： L(\theta)=\prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}其中，m为样本总数，$x^{(i)}$为第i个样本,$y^{(i)}$为第i个样本的类别。对最大似然函数取对数为： l(\theta)=log L(\theta)=\sum_{i=1}^m y^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))其中，满足使$l(\theta)$值最大的$\theta$值，即为所求的参数值。 梯度上升算法 现在如何求满足使$l(\theta)$值最大的$\theta$值呢？这就需要用到梯度上升算法。 根据梯度上升算法可得$\theta$的更新过程： \theta_j=\theta_j+\alpha \frac{\partial{l(\theta)}}{\partial{\theta_j}}其中， $\alpha$为步长。 \begin{align*} \frac{\partial{l(\theta)}}{\partial{\theta_j}}&=\sum_{i=1}^m \left(y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) \frac{\partial{g(\theta^T x^{(i)})}}{\partial{\theta_j}} \\ &=\sum_{i=1}^m \left( y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) g(\theta^T x^{(i)})\left(1-g(\theta^T x^{(i)}) \right) \frac{\partial{\theta^T x^{(i)}}}{\partial{\theta_j}} \\ &= \sum_{i=1}^m \left( y^{(i)}(1-g(\theta^T x^{(i)}))-(1-y^{(i)})g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-h_\theta (x^{(i)}) \right)x^{(i)}_j \end{align*}上述公式求解过程中用到如下换算公式： f(x)=\frac{1}{1+e^{g(x)}}\begin{align*} \frac{\partial{f(x)}}{x}&=\frac{1}{\left( 1+e^{g(x)} \right)^2}e^{g(x)}\frac{\partial{g(x)}}{\partial{x}} \\ &= \frac{1}{1+e^{g(x)}} \frac{e^{g(x)}}{1+e^{g(x)}} \frac{\partial{g(x)}}{\partial{x}} \\ &= f(x)\left(1-f(x)\right) \frac{\partial{g(x)}}{\partial{x}} \end{align*}\frac{\partial{\theta^T x}}{\partial{\theta_j}}=\frac{\partial{(\theta_1 x_1+\cdots+\theta_n x_n)}}{\partial{\theta_j}}=x_j因此，$\theta$的更新过程为： \theta_j=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right) x^{(i)}_j梯度上升过程的向量化 训练数据的矩阵形式如下，x的每一行为一个训练样本，每一列为不同的特征值。 x=\left[ \begin{matrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \end{matrix} \right]= \left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right], y=\left[ \begin{matrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{matrix} \right]参数向量$\theta$的矩阵形式为： \theta=\left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]先求 $x\cdot\theta$并记结果为A: A=x\cdot\theta=\left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right] \cdot \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]= \left[ \begin{matrix} \theta_0 x_0^{(1)}+\theta_1 x_1^{(1)}+\cdots+\theta_n x_n^{(1)} \\ \theta_0 x_0^{(2)}+\theta_1 x_1^{(2)}+\cdots+\theta_n x_n^{(2)} \\ \vdots \\ \theta_0 x_0^{(m)}+\theta_1 x_1^{(m)}+\cdots+\theta_n x_n^{(m)} \\ \end{matrix} \right]再求 $y-h_\theta(x)$ 并记结果为E: E=y-h_\theta(x)= \left[ \begin{matrix} g\left( A^{(1)} \right)-y^{(1)} \\ g\left( A^{(2)} \right)-y^{(2)} \\ \vdots \\ g\left( A^{(m)} \right)-y^{(m)} \end{matrix} \right]= \left[ \begin{matrix} e^{(1)} \\ e^{(2)} \\ \vdots \\ e^{(m)} \end{matrix} \right]= g(A)-y由以上式子可知，$y-h_\theta(x)$ 可由 $g(A)-y$ 一次求得，带入 $\theta$ 的更新过程可得： \begin{align*} \theta_j&=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right)x_j^{(i)} \\ &= \theta_j+\alpha \sum_{i=1}^m e^{(i)} x^{(i)} \\ &= \theta_j+\alpha \left( x_j^{(1)},x_j^{(2)}, \cdots,x_j^{(m)} \right) \cdot E \end{align*}综合起来就是： \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]= \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]+\alpha \cdot \left[ \begin{matrix} x_0^{(1)}&,x_0^{(2)}&,\cdots&,x_0^{(m)} \\ x_1^{(1)}&,x_1^{(2)}&,\cdots&,x_1^{(m)} \\ \vdots&\vdots&\vdots&\vdots \\ x_n^{(1)}&,x_n^{(2)}&,\cdots&,x_n^{(m)} \end{matrix} \right] \cdot E = \theta+\alpha \cdot x^T \cdot EPython代码如下:1234567891011121314151617181920# 阶跃函数def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 使用梯度上升算法计算最佳回归系数def gradientAscent(dataMatIn, classLabels): # 转换为Numpy矩阵类型 dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).T # 矩阵转置 m, n = shape(dataMatrix) alpha = 0.001 # 向目标移动的步长 numCycles = 500 # 迭代次数 weights = ones((n, 1)) # 计算真实类别与预测类别的差值, 按照该差值的方向调整回归系数 for k in range(numCycles): h = sigmoid(dataMatrix * weights) # 矩阵相乘 error = (labelMat - h) # 向量相减 weights = weights + alpha * dataMatrix.T * error return weights 总结《机器学习实战》中短短数十行代码中隐藏了太多的细节，如果不是查阅很多资料，对于初学者来说真的很难弄懂，以上过程还只是最简单的Logistic回归，书上还提供了随机Logistic回归和优化后的随机Logistic回归，看得也是云里雾里 -_-!]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>logistic回归</tag>
        <tag>梯度上升算法</tag>
      </tags>
  </entry>
</search>
