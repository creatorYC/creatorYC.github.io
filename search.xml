<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F05%2F17%2FadaBoost%2F</url>
    <content type="text"><![CDATA[引言AdaBoost, 是“Adaptive Boosting(自适应增强)”的缩写，它是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。 它的自适应性在于：前一个基本分类器分错的样本的权值在下一轮迭代时会上升，分对的样本的权值在下一轮迭代时会降低，加权后的全体样本再次被用来训练下一个基本分类器。每一轮迭代都会产生一个新的基本分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 AdaBoost 算法原理AdaBoost 算法分为三步： 初始化训练数据的权值分布。如果有 N 个样本，则最开始时每个样本的权值都相同，均为： $\frac{1}{N}$。 训练弱分类器。训练过程中，如果某个样本点已经被正确的分类，那么在构造下一个训练集时它的权值就会降低，相反，如果某个样本点分类错误，则它的权值将会升高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代下去。 将每次迭代训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权值，使其在最终的分类函数中起较大的决定作用，减小分类误差率大的弱分类器的权值，使其在最终的分类函数中起较小的决定作用，即误差率低的弱分类器在最终的分类器中占较大的权重。 AdaBoost 算法流程给定一个训练数据集 $ T={(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)} $， 其中，$x \in \chi$，实例空间 $\chi \subset R^n$，$y_i \in {-1, +1}$，算法流程如下： 首先，初始化训练数据的权值分布。每一个训练样本最开始都被赋予相等的权值：$\frac{1}{N}$D_1 = (w_{1,1}, w_{1,2}, \cdots, w_{1,i}, \cdots, w_{1,N}), w_{1,i} = \frac{1}{N}, i=1,2,\cdots,N 进行多次迭代，用 $m = 1,2, \cdots, M$ 表示迭代的轮次。 使用具有权值分布$D_m$的训练集学习，得到基本分类器(选取让误差率最低的阈值来设计基本分类器):G_m(x): x \rightarrow \{-1. +1\} 计算 $G_m(x)$ 在训练数据集上的误差率，e_m = P(G_m(x_i) \neq y_i) = \sum_{G_m(x_i) \neq y_i} w_{mi}由上述式子可知，$G_m(x)$在训练数据集上的误差率 $e_m$ 就是被$G_m(x)$误分类样本的权值之和。 计算 $G_m(x)$ 的系数，$\alpha_m$表示$G_m(x)$在最终的分类器中的重要程度，即得到基本分类器在最终分类器中的权重：\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}由上述式子可知，$e_m \leq \frac{1}{2}$ 时，$\alpha_m \geq 0$，且 $\alpha_m$ 随着 $e_m$ 的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 更新训练数据集的权值分布，得到样本新的权值分布，用于下一轮迭代：D_{m+1} = (w_{m+1,1}, \cdots, w_{m+1,i}, \cdots, w_{m+1, N}),w_{m+1,i} = \frac{w_{m,i}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), i=1,2,\cdots,N其中，$Zm$ 是规范化因子，使得 $D{m+1}$ 成为一个概率分布：Z_m = \sum_{i=1}^N w_{m,i} exp(-\alpha_m y_i G_m(x_i))这样就使得被基本分类器$G_m(x)$误分类样本的权值增大，而被正确分类样本的权值减小。 组合各个弱分类器。 f(x) = \sum_{m=1}^M \alpha_m G_m(x)从而得到最终的分类器如下： G(x) = sign(f(x)) = sign \left( \sum_{m=1}^M \alpha_m G_m(x) \right)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM 初探]]></title>
    <url>%2F2018%2F04%2F28%2FSVM%2F</url>
    <content type="text"><![CDATA[引言SVM — 支持向量机，它是一个二类分类器。在样本数据中，找出一个分割超平面(因为数据集有很多特征，所以是多维的，因此是个分割面)，分布在分割超平面两侧的数据属于不同的类别。为了找出最佳的分割超平面，需要满足一些条件，即离分割超平面距离最近的点的间隔尽可能大。而离分割超平面最近的那些点，就被称为支持向量。现在的目标就是求解这个最佳的分割超平面。 函数间隔与几何间隔分割超平面的方程可以写为：$ \omega^Tx+b=0 $，在分割面的方程确定后，$|\omega^Tx+b|$ 能相对地表示$x$距离超平面的远近，并且，可以通过判断$\omega^Tx+b$的符号与类标记$y$的符号是否一致来表示分类是否正确($y$为1对应$\omega^Tx+b&gt;0$，$y$为-1对应$\omega^Tx+b&lt;0$),所以可以用$y(\omega^Tx+b)$来表示分类的正确性以及某种意义下的距离超平面的距离。这就是所谓的“函数间隔”(这个结果可以保证间隔的值为非负)。但是这样定义的函数间隔有问题，比如说，将$b$和$\omega$成比例的扩大$k$倍，其超平面$k\omega+kb=0$跟之前的超平面是一样的，但是间隔$y(k\omega+kb)$却较之前扩大了$k$倍。因此需要对法向量$\omega$规范化，从而定义“几何间隔”： \gamma = \frac{ y(\omega^Tx+b) }{||\omega||}最大间隔分类器对于一个数据点进行分类时，离分割面的距离越大越可信。对于一个包含n个点的数据集，间隔为n个点中最小的那个，为了使得分类的可信度高，我们选择的超平面应该最大化这个间隔。因此我们的目标函数为： \max\ \gamma需要满足的限制条件为： y_i (\omega^T x_i+b) = \gamma_i \geq \gamma , i=1,\cdots,n为了方便推导，令函数间隔$ y(\omega^Tx+b) = 1$，因此，目标函数为： \max\ \frac{1}{||\omega||} , s.t. \ y_i (\omega^T x_i+b) \geq 1这个问题等价于（为了方便求解，加上了平方，还有一个系数）： \min\ \frac{1}{2} ||\omega||^2 , s.t. \ y_i (\omega^T x_i+b) \geq 1这是一个二次优化问题——目标函数是二次的，约束条件是线性的，这个优化问题可以用拉格朗日乘数法求解(大学学的数学还真的很有用，哈哈~)，因此，最终的函数形式为： L(\omega, b, \alpha) = \frac{1}{2} ||\omega||^2 - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1)然后我们令： \theta(\omega) = \max \limits_{\alpha_i \geq 0} L(\omega, b, \alpha)容易验证，当某个约束条件不满足时，例如 $ y_i(\omega^Tx_i+b) \le 1 $，那么我们显然有 $ \theta(\omega) = \infty $，而当所有约束条件都满足时，则有 $ \theta(\omega) = \frac{1}{2} ||\omega||^2 $ ，即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2} ||\omega||^2$ 实际上等价于直接最小化$\theta(\omega)$。现在，目标函数变成了： \min \limits_{\omega, b} \theta(\omega) = \min \limits_{\omega, b} \max \limits_{\alpha_i \geq 0}L(\omega, b, \alpha) = P这里用 $P$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。现在把最小和最大的位置交换一下： \max \limits_{\alpha_i \geq 0} \min \limits_{\omega, b} L(\omega, b, \alpha) = D交换以后的问题不再等价于原问题，这个新问题的最优值用 $D$来表示。并且有 $D \leq P $，(这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！) 总之，第二个问题的最优值 $D$在这里提供了一个第一个问题的最优值 $P$的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。现在来求解这个问题。先让 $L$ 最小化，直接让$L$对$\omega$和$b$求导， \frac{\partial L}{\partial \omega} = \omega - \sum_{i=1}^n \alpha_i y_i x_i = 0 \Rightarrow \omega = \sum_{i=1}^n \alpha_i y_i x_i\frac{\partial L}{\partial b} = - \sum_{i=1}^n \alpha_i y_i = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0带回 $L$ 得到： \begin{align*} L(\omega, b, \alpha) &= \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ &= \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \end{align*}此时得到关于 $\alpha$ 的优化问题： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_js.t. \alpha_i \geq 0, i=1,\cdots, n , \sum_{i=1}^n \alpha_i y_i = 0我们对一个数据点$x$进行分类的时候，实际上是通过将$x$代入到 $f(x)=\omega^Tx+b$，根据结果的正负来判断$x$所属的分类，而 $\omega = \sum_{i=1}^n \alpha_i y_i x_i$，因此： f(X) = (\sum_{i=1}^n \alpha_i y_i x_i)^T x + b = \sum_{i=1}^n \alpha_i y_i \langle x_i, x \rangle + b这里有趣之处在于，对于新点$x$的预测，只需要计算它与训练数据点的内积即可。另外，所谓的支持向量也在这里显示出来—事实上，所有的非支持向量所对应的系数$\alpha$都为0，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。为什么非支持向量对应的$\alpha$等于零呢？之前的目标函数为： \max \limits_{\alpha_i \geq 0} L(\omega, b, \alpha) = \max \limits_{\alpha_i \geq 0} \frac{1}{2} ||\omega||^2 - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1)注意到如果$x_i$是支持向量的话，上式中$(y_i(\omega^Tx_i+b)-1)$部分是等于 0 的（因为支持向量的函数间隔等于 1 ），而对于非支持向量来说，函数间隔会大于 1 ，因此$(y_i(\omega^Tx_i+b)-1)$是大于零的，而$\alpha_i$又是非负的，为了满足最大化，$\alpha_i$必须等于 0 。 Kernel前面介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。但是大部分数据集都是线性不可分的，因此，对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。前面我们得到的分类函数为： f(x) = \sum_{i=1}^n \alpha_i y_i \langle x_i, x \rangle + b现在则为映射后的空间，即： f(x) = \sum_{i=1}^n \alpha_i y_i \langle \phi(x_i), \phi(x) \rangle + b因此，建立非线性学习器分为两步： 首先使用一个非线性映射将数据变换到一个特征空间 然后在特征空间使用线性学习器分类。 因为直接将低维空间映射到高维空间的函数$\phi(x)$可能很难计算，因此如果有一种方式可以在特征空间中直接计算内积 $\langle \phi(x_i), \phi(x) \rangle$，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个非线性的学习器，这样直接计算法的方法称为核函数方法： K(x, z) = \langle \phi(x), \phi(z) \rangle核函数能简化映射空间中的内积运算。现在，分类函数为： \sum_{i=1}^n \alpha_i y_i K(x,x_i) + b其中 $\alpha$ 由如下问题计算而得： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j)s.t. \alpha_i \geq 0, i=1,\cdots, n , \sum_{i=1}^n \alpha_i y_i = 0这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算。核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。对于任意一个映射，构造出合适的核函数很困难，因此通常人们会从一些常用的核函数中选择，如： 高斯核函数：K(x1, x2) = exp \left( -\frac{||x1-x2||^2}{2\sigma^2} \right)这是一个采用 向量 作为自变量的函数，能够基于向量距离运算输出一个标量，其中，$\sigma$ 是用户定义的用于确定到达率或者函数值跌落到0的速度参数，高斯核函数将数据从其特征空间映射到无穷维空间。 Outliers如果不是因为数据本身是非线性结构的，而只是因为数据有噪音，对于这种偏离正常位置很远的数据点(outlier) ，在我们原来的 SVM 模型里，可能造成很大的影响。为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。具体来说，原来的约束条件为： y_i(\omega^T x_i+b) \geq 1, i=1,\cdots,n现在变为： y_i(\omega^T x_i+b) \geq 1-\xi_i, i=1,\cdots,n其中 $\xi_i \geq 0$ 称为松弛变量(slack variable)，对应数据点$x_i$允许偏离的函数间隔的量。当然，如果我们任由 $\xi_i$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi_i$的总和也要最小： \min \frac{1}{2} ||\omega^2|| + C\sum_{i=1}^n \xi_i其中$C$是一个参数，用于控制目标函数中两项（“寻找间隔最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中$\xi_i$是需要优化的变量，而$C$是一个事先确定好的常量。完整地写出来为： \min \frac{1}{2} ||\omega^2|| + C\sum_{i=1}^n \xi_is.t. y_i(\omega^T x_i+b) \geq 1-\xi_i, i=1,\cdots,n\xi_i \geq 0, i=1,\cdots,n用之前的方法将限制加入到目标函数中，为： L(\omega, b, \alpha) = \frac{1}{2} ||\omega||^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i (y_i(\omega^Tx_i+b)-1+\xi_i) - \sum_{i=1}^n \gamma_i \xi_i像前面一样，先求导： \frac{\partial L}{\partial \omega}=0 \Rightarrow \omega=\sum_{i=1}^n \alpha_i y_i x_i\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0\frac{\partial L}{\partial \xi_i}=0 \Rightarrow C-\alpha_i-\gamma_i=0, i=1,\cdots,n将 $\omega$带回$L$并化简可得到与之前一样的目标函数： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangle不过，因为有 $ C-\alpha_i-\gamma_i=0 $ 和 $ \gamma_i \geq 0 $，因此有 $ C \geq \alpha_i \geq 0 $，所以最终问题写为： \max \limits_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangles.t. 0 \leq \alpha_i \leq C, i=1,\cdots,n\sum_{i=1}^n \alpha_i y_i = 0这与之前问题的唯一区别就是$\alpha$多了一个上限$C$。现在，一个完整的、可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机终于介绍完毕了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic 回归与梯度上升算法]]></title>
    <url>%2F2018%2F04%2F23%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[引言最近在看《机器学习实战》，这本书将所有的算法都用 Python 代码实现了，非常具有实战性，但对于算法原理的讲解则比较粗略，也没有详细的数学推导过程，所以很多哪怕几行简单的代码，也得找资料理解好久。 Logistic 回归与梯度上升算法Logistic 回归 Logistic 回归用于二分类问题，根据现有数据对分类边界线建立回归公式，并以此进行分类。Logistic 回归本质上是一个基于条件概率的判别模型，利用了 Sigmoid 函数值域在(0, 1)之间的特性。Sigmoid 函数的计算公式为： g(z)=\frac{1}{1+e^{-z}}通过 Sigmid 函数计算出结果，若结果大于0.5，则为类别1，反之属于类别0。由于数据有多个特征，因此函数的输入用向量表示如下： \theta_0+\theta_1x_1+\cdots+\theta_nx_n=\sum_{i=0}^{n} {\theta_ix_i}=\theta^Tx则函数计算公式变为： h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}如果我们求出了合适的参数向量 $\theta$，以及样本x，那么就可以根据上述公式，计算出概率值，从而判断x所属的分类。 那么如何求出合适的参数向量$\theta$呢？ 上述 $h_\theta(x)$ 函数具有特殊含义，它表示结果取1的概率，因此对于输入的x分类结果为类别1和类别0的概率为： P(y=1|x;\theta)=h_\theta(x)P(y=0|x;\theta)=1-h_\theta(x)将两个公式合并为一个即为： P(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}取最大似然函数为： L(\theta)=\prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}其中，m为样本总数，$x^{(i)}$为第i个样本,$y^{(i)}$为第i个样本的类别。对最大似然函数取对数为： l(\theta)=log L(\theta)=\sum_{i=1}^m y^{(i)}log h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))其中，满足使$l(\theta)$值最大的$\theta$值，即为所求的参数值。 梯度上升算法 现在如何求满足使$l(\theta)$值最大的$\theta$值呢？这就需要用到梯度上升算法。 根据梯度上升算法可得$\theta$的更新过程： \theta_j=\theta_j+\alpha \frac{\partial{l(\theta)}}{\partial{\theta_j}}其中， $\alpha$为步长。 \begin{align*} \frac{\partial{l(\theta)}}{\partial{\theta_j}}&=\sum_{i=1}^m \left(y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) \frac{\partial{g(\theta^T x^{(i)})}}{\partial{\theta_j}} \\ &=\sum_{i=1}^m \left( y^{(i)}\frac{1}{g(\theta^T x^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^T x^{(i)})} \right) g(\theta^T x^{(i)})\left(1-g(\theta^T x^{(i)}) \right) \frac{\partial{\theta^T x^{(i)}}}{\partial{\theta_j}} \\ &= \sum_{i=1}^m \left( y^{(i)}(1-g(\theta^T x^{(i)}))-(1-y^{(i)})g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-g(\theta^T x^{(i)}) \right)x^{(i)}_j \\ &= \sum_{i=1}^m \left( y^{(i)}-h_\theta (x^{(i)}) \right)x^{(i)}_j \end{align*}上述公式求解过程中用到如下换算公式： f(x)=\frac{1}{1+e^{g(x)}}\begin{align*} \frac{\partial{f(x)}}{x}&=\frac{1}{\left( 1+e^{g(x)} \right)^2}e^{g(x)}\frac{\partial{g(x)}}{\partial{x}} \\ &= \frac{1}{1+e^{g(x)}} \frac{e^{g(x)}}{1+e^{g(x)}} \frac{\partial{g(x)}}{\partial{x}} \\ &= f(x)\left(1-f(x)\right) \frac{\partial{g(x)}}{\partial{x}} \end{align*}\frac{\partial{\theta^T x}}{\partial{\theta_j}}=\frac{\partial{(\theta_1 x_1+\cdots+\theta_n x_n)}}{\partial{\theta_j}}=x_j因此，$\theta$的更新过程为： \theta_j=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right) x^{(i)}_j梯度上升过程的向量化 训练数据的矩阵形式如下，x的每一行为一个训练样本，每一列为不同的特征值。 x=\left[ \begin{matrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \end{matrix} \right]= \left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right], y=\left[ \begin{matrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{matrix} \right]参数向量$\theta$的矩阵形式为： \theta=\left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]先求 $x\cdot\theta$并记结果为A: A=x\cdot\theta=\left[ \begin{matrix} x_0^{(1)} & x_1^{(1)} & \cdots & x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ x_0^{(m)} & x_1^{(m)} & \cdots & x_n^{(m)} \\ \end{matrix} \right] \cdot \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{matrix} \right]= \left[ \begin{matrix} \theta_0 x_0^{(1)}+\theta_1 x_1^{(1)}+\cdots+\theta_n x_n^{(1)} \\ \theta_0 x_0^{(2)}+\theta_1 x_1^{(2)}+\cdots+\theta_n x_n^{(2)} \\ \vdots \\ \theta_0 x_0^{(m)}+\theta_1 x_1^{(m)}+\cdots+\theta_n x_n^{(m)} \\ \end{matrix} \right]再求 $y-h_\theta(x)$ 并记结果为E: E=y-h_\theta(x)= \left[ \begin{matrix} g\left( A^{(1)} \right)-y^{(1)} \\ g\left( A^{(2)} \right)-y^{(2)} \\ \vdots \\ g\left( A^{(m)} \right)-y^{(m)} \end{matrix} \right]= \left[ \begin{matrix} e^{(1)} \\ e^{(2)} \\ \vdots \\ e^{(m)} \end{matrix} \right]= g(A)-y由以上式子可知，$y-h_\theta(x)$ 可由 $g(A)-y$ 一次求得，带入 $\theta$ 的更新过程可得： \begin{align*} \theta_j&=\theta_j+\alpha \sum_{i=1}^m \left( y^{(i)}-h_\theta(x^{(i)}) \right)x_j^{(i)} \\ &= \theta_j+\alpha \sum_{i=1}^m e^{(i)} x^{(i)} \\ &= \theta_j+\alpha \left( x_j^{(1)},x_j^{(2)}, \cdots,x_j^{(m)} \right) \cdot E \end{align*}综合起来就是： \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]= \left[ \begin{matrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \\ \end{matrix} \right]+\alpha \cdot \left[ \begin{matrix} x_0^{(1)}&,x_0^{(2)}&,\cdots&,x_0^{(m)} \\ x_1^{(1)}&,x_1^{(2)}&,\cdots&,x_1^{(m)} \\ \vdots&\vdots&\vdots&\vdots \\ x_n^{(1)}&,x_n^{(2)}&,\cdots&,x_n^{(m)} \end{matrix} \right] \cdot E = \theta+\alpha \cdot x^T \cdot EPython代码如下:1234567891011121314151617181920# 阶跃函数def sigmoid(inX): return 1.0 / (1 + exp(-inX))# 使用梯度上升算法计算最佳回归系数def gradientAscent(dataMatIn, classLabels): # 转换为Numpy矩阵类型 dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).T # 矩阵转置 m, n = shape(dataMatrix) alpha = 0.001 # 向目标移动的步长 numCycles = 500 # 迭代次数 weights = ones((n, 1)) # 计算真实类别与预测类别的差值, 按照该差值的方向调整回归系数 for k in range(numCycles): h = sigmoid(dataMatrix * weights) # 矩阵相乘 error = (labelMat - h) # 向量相减 weights = weights + alpha * dataMatrix.T * error return weights 总结《机器学习实战》中短短数十行代码中隐藏了太多的细节，如果不是查阅很多资料，对于初学者来说真的很难弄懂，以上过程还只是最简单的Logistic回归，书上还提供了随机Logistic回归和优化后的随机Logistic回归，看得也是云里雾里 -_-!]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>logistic回归</tag>
        <tag>梯度上升算法</tag>
      </tags>
  </entry>
</search>
